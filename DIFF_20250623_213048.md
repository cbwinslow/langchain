diff --git a/docs/embedding_models_overview.md b/docs/embedding_models_overview.md
new file mode 100644
index 000000000..22a29580e
--- /dev/null
+++ b/docs/embedding_models_overview.md
@@ -0,0 +1,10 @@
+# Embedding Models Overview
+
+This brief summary lists common embeddings for Retrieval-Augmented Generation (RAG) systems. The examples in this repository default to the MiniLM model from HuggingFace, but other models may be used depending on use case.
+
+- **HuggingFace Transformer models** – wide variety of open-source models for general text. See the LangChain docs for usage examples.
+- **OpenAI embeddings** – powerful embeddings accessible via API for production workloads.
+- **Cohere embeddings** – suited for multilingual and long context data.
+- **FakeEmbeddings** – lightweight model from LangChain core for testing without network downloads.
+
+Refer to the documentation notebooks under `docs/docs/how_to/indexing.ipynb` and the integration guides in `docs/docs/integrations/` for detailed tutorials.
diff --git a/examples/rag_webapp/app.py b/examples/rag_webapp/app.py
index ab290d788..3651ba54a 100644
--- a/examples/rag_webapp/app.py
+++ b/examples/rag_webapp/app.py
@@ -6,6 +6,7 @@ from pydantic import BaseModel
 from langchain.chains import RetrievalQA
 from langchain_community.vectorstores import Chroma
 from langchain_huggingface import HuggingFaceEmbeddings
+from langchain_core.embeddings import FakeEmbeddings
 from langchain_community.llms import HuggingFaceHub
 
 try:
@@ -21,7 +22,11 @@ except Exception:  # pragma: no cover
 app = FastAPI()
 
 db_dir = os.environ.get("PERSIST_DIR", "chroma_db")
-embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
+embedding_model = os.environ.get("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")
+if embedding_model == "fake":
+    embedding = FakeEmbeddings(size=10)
+else:
+    embedding = HuggingFaceEmbeddings(model_name=embedding_model)
 
 store_type = os.environ.get("VECTOR_STORE_TYPE", "chroma")
 if store_type == "chroma":
diff --git a/examples/rag_webapp/ingest.py b/examples/rag_webapp/ingest.py
index 6541d254e..8f20a3807 100644
--- a/examples/rag_webapp/ingest.py
+++ b/examples/rag_webapp/ingest.py
@@ -12,6 +12,7 @@ from pathlib import Path
 from langchain_community.document_loaders import DirectoryLoader, TextLoader
 from langchain.text_splitters import RecursiveCharacterTextSplitter
 from langchain_huggingface import HuggingFaceEmbeddings
+from langchain_core.embeddings import FakeEmbeddings
 from langchain_community.vectorstores import Chroma
 
 try:  # Optional imports for other stores
@@ -25,11 +26,17 @@ except Exception:  # pragma: no cover
     Weaviate = None
 
 
-def ingest(source_dir: str, persist_dir: str = "db", store_type: str = "chroma") -> None:
+def ingest(
+    source_dir: str,
+    persist_dir: str = "db",
+    store_type: str = "chroma",
+    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2",
+) -> None:
     """Ingest markdown docs into a selected vector store.
 
-    Parameters are inspired by examples in LangChain docs.``store_type`` can be
-    ``chroma``, ``elastic`` or ``weaviate``.
+    Parameters are inspired by examples in LangChain docs. ``store_type`` can be
+    ``chroma``, ``elastic`` or ``weaviate``. ``embedding_model`` selects the
+    HuggingFace model or ``fake`` for testing.
     """
     loader = DirectoryLoader(
         source_dir,
@@ -40,10 +47,13 @@ def ingest(source_dir: str, persist_dir: str = "db", store_type: str = "chroma")
     docs = loader.load()
     splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
     splits = splitter.split_documents(docs)
-    embeddings = HuggingFaceEmbeddings(
-        model_name="sentence-transformers/all-MiniLM-L6-v2",
-        model_kwargs={"device": "cpu"},
-    )
+    if embedding_model == "fake":
+        embeddings = FakeEmbeddings(size=10)
+    else:
+        embeddings = HuggingFaceEmbeddings(
+            model_name=embedding_model,
+            model_kwargs={"device": "cpu"},
+        )
     if store_type == "chroma":
         vectordb = Chroma.from_documents(splits, embeddings, persist_directory=persist_dir)
     elif store_type == "elastic" and ElasticsearchStore is not None:
@@ -62,6 +72,9 @@ def ingest(source_dir: str, persist_dir: str = "db", store_type: str = "chroma")
     else:
         raise ValueError(f"Unsupported store type: {store_type}")
     vectordb.persist()
+    print(
+        f"Ingested {len(splits)} documents into {store_type} store at {persist_dir}"
+    )
 
 
 def main() -> None:
@@ -69,10 +82,16 @@ def main() -> None:
     parser.add_argument("--source", default=os.environ.get("SOURCE_DIR", "docs"))
     parser.add_argument("--persist", default=os.environ.get("PERSIST_DIR", "chroma_db"))
     parser.add_argument("--store", default=os.environ.get("VECTOR_STORE_TYPE", "chroma"))
+    parser.add_argument(
+        "--embedding",
+        default=os.environ.get(
+            "EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2"
+        ),
+    )
     args = parser.parse_args()
 
     Path(args.persist).mkdir(parents=True, exist_ok=True)
-    ingest(args.source, args.persist, args.store)
+    ingest(args.source, args.persist, args.store, args.embedding)
 
 
 if __name__ == "__main__":
diff --git a/tests/test_rag_webapp.py b/tests/test_rag_webapp.py
new file mode 100644
index 000000000..c93aa24f6
--- /dev/null
+++ b/tests/test_rag_webapp.py
@@ -0,0 +1,26 @@
+import os
+from pathlib import Path
+from fastapi.testclient import TestClient
+
+from examples.rag_webapp import ingest
+
+
+def test_ingest_and_query(tmp_path: Path, monkeypatch):
+    source = tmp_path / "docs"
+    source.mkdir()
+    (source / "test.md").write_text("Hello world")
+
+    db_dir = tmp_path / "db"
+    ingest.ingest(str(source), str(db_dir), store_type="chroma", embedding_model="fake")
+
+    monkeypatch.setenv("PERSIST_DIR", str(db_dir))
+    monkeypatch.setenv("VECTOR_STORE_TYPE", "chroma")
+    monkeypatch.setenv("EMBEDDING_MODEL", "fake")
+
+    from examples.rag_webapp import app as web_app
+
+    client = TestClient(web_app.app)
+    resp = client.post("/query", json={"question": "Hello?"})
+    assert resp.status_code == 200
+    assert "answer" in resp.json()
+
